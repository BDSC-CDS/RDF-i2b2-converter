import os, sys, pytest

myPath = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, myPath + "/../src/")
from ontology_converter import *
from load_data import *


def test_formatvname():
    vname = "@SNOMEDCT code child of 1234 |fnjfe|"
    res = format_vsetdescr(vname)

    assert res == "@SNOMEDCT.1234"


def test_easydb():
    classes = setup()
    db = gather_onto_lines(classes)
    return db


def test_getlabels():
    dics = ["SNOMEDCT.284009009", "ICD10", "ATC"]
    res = []
    for el in dics:
        res.append(extend_valuepath(el))

    # Not much to test except that the resulting objects are valid dictionaries
    assert all([type(el) == dict for el in res])


def test_unit_search():
    db = test_easydb()


def test_ontodb():
    db = test_easydb()
    extract_C_M_dim(db)
    db_csv(db, "spo_onto_db")


def test_fillnodes():
    import split_nodes_data


def test_conceptdim():
    db = test_easydb()
    extract_C_M_dim(db)
    conceptdb = from_csv("CONCEPT_DIMENSION.csv")
    for line in conceptdb:
        if line["name_char"] in CONCEPT_BLACKLIST + ABSTRACT_CLASSES:
            assert False
            return
    assert True


def test_modifdim():
    # Check all modifiers in the modifier dimension reference a valid concept code
    modifdb = from_csv("MODIFIER_DIMENSION.csv")
    concepts = from_csv("CONCEPT_DIMENSION.csv")
    codes = [el["concept_cd"] for el in concepts]
    for line in modifdim:
        if line["m_applied_path"] not in codes:
            assert False
    assert True


def test_refilldb():
    filename = "minionto"
    toy = {"\\A\\AB\\ABC": False, "\\A": True, "\\1\\12\\123\\1234": False}
    with open("ontologies/" + filename, "w") as ff:
        json.dump(toy, ff)
    fill_db_holes(filename)
    assert fill_db_holes(filename) == {}


def test_labels(ontodic):
    # TODO : find out why 404 error on some links
    counter = 0
    # Get all ontologies from the REST service and parse the JSON
    ontology = get_json(REST_URL + "/ontologies/" + ontodic)
    # Iterate looking for ontology with acronym BRO

    labels = {}
    # Using the hypermedia link called `classes`, get the first page
    if "classes" in ontology["links"].keys():
        page = get_json(ontology["links"]["classes"])
    # In the case our root is not the root of an ontology but of a subtree, slightly different call:
    else:
        page = get_json(ontology["links"]["descendants"])

    # Iterate over the available pages adding labels from all classes
    # When we hit the last page, the while loop will exit
    next_page = page
    while next_page:
        next_page = page["links"]["nextPage"]
        for bro_class in page["collection"]:
            print("new item")
        if next_page:
            counter = counter + 1
            print("next page..." + str(counter))
            page = get_json(next_page)
    # Output the labels
    pdb.set_trace()
    return labels


def test_loaddata_properties():
    g, lis = navigate_graph()
    res = g.resource(lis[0][0])
    met = extract_metainfo(res)


def test_dataloading():
    instances = navigate_graph()
    db = generate_dbblock(instances)
    db_csv(db, "observation_fact_spo")
    assert check_starschema(db)


def test_lostkid():
    kids = read_lostkids("ICD10")
    labels = read_ontodic("ICD10")
    save = labels.copy()
    kkeys = [k for k in kids.keys()]
    for key in kkeys:
        value = kids[key]
        labels.update(correct_position(key, value, labels))
    pdb.set_trace()
    return labels


def test_topdown_bio(dicname):
    url = REST_URL + "/ontologies/" + dicname
    root = get_json(url)
    levelones = get_json(root["links"]["children"])

    resdict = discover_tree_topdown(levelones, prefix="\\" + root["prefLabel"])
    resdict.update({"\\" + root["prefLabel"]: True})
    pdb.set_trace()


def test_matchcodes():
    """
    test if the codes generated by the data loader match the codes stored in concept and modifier dimension.
    """
    print("Number of valueset classes is " + str(len(HAVE_VALUESET)))
    keys = navigate_graph()
    db = []
    c_dicts = from_csv("CONCEPT_DIMENSION.csv")
    m_dicts = from_csv("MODIFIER_DIMENSION.csv")
    c_codes = [el["concept_cd"] for el in c_dicts]
    m_codes = [el["modifier_cd"] for el in m_dicts]

    dataccodes = []
    datamcodes = []
    for observation in keys:
        concept_line = {"name": observation.value(RDF.type).identifier.toPython()}
        # TODO solve this, we shouldnt have to make this continue test
        if (
            "DataP" in concept_line["name"]
            or "Subject" in concept_line["name"]
            or "Encounter" in concept_line["name"]
        ):
            continue

        concept_code = reduce_basecode(concept_line["name"])
        concept_line.update({"code": concept_code})
        dataccodes.append(concept_code)
        """ if concept_code not in c_codes :
            pdb.set_trace()
            raise Exception("concept code not found") """
        modifier_lines = dig_attributes(observation)
        concept_line.update(modifier_lines[0])
        modifier_lines = modifier_lines[1:]
        for mod in modifier_lines:
            datamcodes.append(mod["code"])
            if not mod["code"] in m_codes:
                pdb.set_trace()
                raise Exception("modif code not found")

    pdb.set_trace()
    assert all([el in c_codes for el in dataccodes])
    assert all([el in m_codes for el in datamcodes])
